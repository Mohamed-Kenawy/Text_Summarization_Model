# Text Summarization pipeline ğŸ“°âœ‚ï¸ğŸ¤–

## ğŸ“Œ Overview  
Text Summarization with T5 This project implements an abstractive text summarization pipeline using the T5 Transformer model. It trains and evaluates on the CNN/DailyMail dataset, with evaluation metrics including ROUGE and BERTScore. The notebook covers data preprocessing, model fine-tuning, evaluation.

## ğŸ“‚ Dataset  
The project uses the **CNN/DailyMail dataset**:  
- `train.csv` â†’ Training articles & summaries  
- `validation.csv` â†’ Validation set  
- `test.csv` â†’ Test set
- Data link: https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail

## ğŸ“Preprocessing
- Keep only essential columns: `article`, `highlights`  
- Add prefix `"summarize:"` to input text (T5 requirement)  
- Tokenize inputs and summaries  
- Truncate / pad sequences for uniform length  

## âš™ï¸ Model Used  
- **T5-small** (fine-tuned for summarization)  
- Can be scaled to **T5-base** or **T5-large** for better performance  

## ğŸ“Š Feature Extraction  
- Tokenization with Hugging Face `T5Tokenizer`  
- Padding & truncation for long sequences  
- Seq2Seq training with Hugging Face `Trainer`  

## ğŸ“ˆ Evaluation Metrics  
- **ROUGE Score** â†’ n-gram overlap with reference summary  
- **BERTScore** â†’ semantic similarity  


## ğŸ“¥ Clone Repository  
```bash
git clone https://github.com/Mohamed-Kenawy/Text_Summarization_pipeline.git
cd Text_Summarization_pipeline
