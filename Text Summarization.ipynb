{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bcb6b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\Ai\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Import Required Libraries ----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer, \n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    DataCollatorForSeq2Seq,\n",
    "    pipeline\n",
    ")\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas()\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a66d5980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n",
       "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
       "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n",
       "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
       "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n",
       "      <td>A drunk driver who killed a young woman in a h...</td>\n",
       "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n",
       "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
       "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n",
       "      <td>Fleetwood are the only team still to have a 10...</td>\n",
       "      <td>Fleetwood top of League One after 2-0 win at S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
       "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
       "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
       "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
       "4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
       "\n",
       "                                             article  \\\n",
       "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
       "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
       "2  A drunk driver who killed a young woman in a h...   \n",
       "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
       "4  Fleetwood are the only team still to have a 10...   \n",
       "\n",
       "                                          highlights  \n",
       "0  Bishop John Folda, of North Dakota, is taking ...  \n",
       "1  Criminal complaint: Cop used his role to help ...  \n",
       "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
       "3  Nina dos Santos says Europe must be ready to a...  \n",
       "4  Fleetwood top of League One after 2-0 win at S...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- Load Dataset ----\n",
    "train_data = pd.read_csv(\"cnn_dailymail/train.csv\")\n",
    "val_data   = pd.read_csv(\"cnn_dailymail/validation.csv\")\n",
    "test_data  = pd.read_csv(\"cnn_dailymail/test.csv\")\n",
    "\n",
    "display(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b4f3200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Train: 0\n",
      "Missing values in Validation: 0\n",
      "Missing values in Test: 0\n"
     ]
    }
   ],
   "source": [
    "# ---- Data Cleaning Check ----\n",
    "print(\"Missing values in Train:\", train_data.isnull().sum().sum())\n",
    "print(\"Missing values in Validation:\", val_data.isnull().sum().sum())\n",
    "print(\"Missing values in Test:\", test_data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d36d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Take a random sample ---\n",
    "train_data_sample = train_data.sample(frac=0.05, random_state=42)  # change frac as needed (0.1 = 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca9495c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Convert DataFrames to Hugging Face Dataset ----\n",
    "# Keeping only necessary columns: [\"article\", \"highlights\"]\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data_sample[[\"article\", \"highlights\"]])\n",
    "val_dataset   = Dataset.from_pandas(val_data[[\"article\", \"highlights\"]])\n",
    "test_dataset  = Dataset.from_pandas(test_data[[\"article\", \"highlights\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd0c05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# ---- Load T5 Model & Tokenizer ----\n",
    "# Use t5-small for faster training, upgrade to t5-base or t5-large if GPU allows\n",
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "572ba289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14356/14356 [00:27<00:00, 513.52 examples/s]\n",
      "Map: 100%|██████████| 13368/13368 [00:25<00:00, 526.64 examples/s]\n",
      "Map: 100%|██████████| 11490/11490 [00:21<00:00, 523.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ---- Preprocessing Function ----\n",
    "# Adds task prefix \"summarize:\" for T5\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"article\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=max_input_length, \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        examples[\"highlights\"], \n",
    "        max_length=max_target_length, \n",
    "        truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_tokenized = train_dataset.map(preprocess_function, batched=True)\n",
    "val_tokenized   = val_dataset.map(preprocess_function, batched=True)\n",
    "test_tokenized  = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6be57714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Data Collator ----\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1c57198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b907765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Define Trainer ----\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "717c5514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1436' max='1436' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1436/1436 3:10:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.293300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.132400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.086300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.081700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.045500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1436, training_loss=2.103871836967787, metrics={'train_runtime': 11411.6893, 'train_samples_per_second': 1.258, 'train_steps_per_second': 0.126, 'total_flos': 1942966901932032.0, 'train_loss': 2.103871836967787, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Train the Model ----\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3848eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1149' max='1149' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1149/1149 29:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8726658821105957, 'eval_runtime': 1785.5014, 'eval_samples_per_second': 6.435, 'eval_steps_per_second': 0.644, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# ---- Evaluate on Test Set ----\n",
    "results = trainer.evaluate(test_tokenized)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01f3ed41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Article:\n",
      " Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by t ...\n",
      "\n",
      "✅ Prediction:\n",
      " A U.S consumer advisory group set up by the Department of Transportation said that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans. But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased. The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch.\n",
      "\n",
      "📝 Reference:\n",
      " Experts question if  packed out planes are putting passengers at risk .\n",
      "U.S consumer advisory group says minimum space must be stipulated .\n",
      "Safety tests conducted on planes with more leg room than airlines offer .\n",
      "\n",
      "📊 ROUGE: {'rouge1': Score(precision=0.1839080459770115, recall=0.47058823529411764, fmeasure=0.2644628099173554), 'rouge2': Score(precision=0.06976744186046512, recall=0.18181818181818182, fmeasure=0.10084033613445377), 'rougeL': Score(precision=0.13793103448275862, recall=0.35294117647058826, fmeasure=0.1983471074380165)}\n"
     ]
    }
   ],
   "source": [
    "# ---- ROUGE Evaluation ----\n",
    "# Decode predictions for test samples and calculate ROUGE\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "def compute_rouge(sample_idx=0):\n",
    "    article = test_data[\"article\"].iloc[sample_idx]\n",
    "    reference = test_data[\"highlights\"].iloc[sample_idx]\n",
    "    \n",
    "    # Generate summary\n",
    "    input_ids = tokenizer(\"summarize: \" + article, return_tensors=\"pt\", truncation=True).input_ids\n",
    "    output_ids = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)\n",
    "    prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Compute ROUGE\n",
    "    scores = scorer.score(reference, prediction)\n",
    "    print(\"📌 Article:\\n\", article[:500], \"...\")\n",
    "    print(\"\\n✅ Prediction:\\n\", prediction)\n",
    "    print(\"\\n📝 Reference:\\n\", reference)\n",
    "    print(\"\\n📊 ROUGE:\", scores)\n",
    "\n",
    "compute_rouge(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f9416ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 222.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.13 seconds, 4.70 sentences/sec\n",
      "BERTScore F1: 0.8804254531860352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- BERTScore Evaluation ----\n",
    "# More semantic evaluation compared to ROUGE\n",
    "cands = [tokenizer.decode(model.generate(\n",
    "    tokenizer(\"summarize: \" + art, return_tensors=\"pt\", truncation=True).input_ids,\n",
    "    max_length=128, num_beams=4, early_stopping=True\n",
    ")[0], skip_special_tokens=True) for art in test_data[\"article\"].head(10)]\n",
    "\n",
    "refs = test_data[\"highlights\"].head(10).tolist()\n",
    "\n",
    "P, R, F1 = score(cands, refs, lang=\"en\", verbose=True)\n",
    "print(\"BERTScore F1:\", F1.mean().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
